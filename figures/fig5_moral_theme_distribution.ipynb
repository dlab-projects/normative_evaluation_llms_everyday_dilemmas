{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb87f30-ea31-4e1c-919b-80cd21081f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_lego as mplego\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from mpl_lego.labels import bold_text\n",
    "from pyprojroot import here\n",
    "\n",
    "from normative_evaluation_llms_everyday_dilemmas import keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4a1bc-2d30-4ee4-925a-350027db5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mplego.style.use_latex_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e5441-2451-4ff4-957d-94b19a05743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(here('data/normative_evaluation_everyday_dilemmas_dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828fb8ce-e3f5-4fa6-93ba-a24a9e6f48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_labels_df = df[keys.MORAL_LABELS_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28718cc-ef7e-4b13-ad4b-684bcf1a1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_model_cols_dict = {model: [f'{reason_col}_{moral_axis}_label'\n",
    "  for moral_axis in keys.MORAL_AXES]\n",
    "  for model, reason_col in zip(keys.MODELS, keys.REASON_COLS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d106eabf-affa-4952-9796-86b64da14be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_labels_df.sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17836713-4e9a-415d-a239-9e9f9ef98bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_labels_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2fb6f-07e0-4455-8f91-bc575b19b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'harm_prob': 'harms_prob', 'harm_label': 'harms_label'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b76180-69f1-4062-b538-ab576155560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "jitter = np.linspace(-0.35, 0.35, len(keys.MODELS))\n",
    "width = jitter[1] - jitter[0]\n",
    "ax.set_xlim([-0.7, 5.7])\n",
    "for idx, moral_axis in enumerate(keys.MORAL_AXES):\n",
    "    cols = [f'{model}_{moral_axis}_label' for model in keys.REASON_COLS]\n",
    "    ax.bar(idx + jitter, df[cols].mean(axis=0).values,\n",
    "           color=[f'C{idx}' for idx in range(len(keys.MODELS))],\n",
    "           width=width)\n",
    "    xmin = idx + jitter[0]\n",
    "    xmax = idx + jitter[-1]\n",
    "    x_min_lim, x_max_lim = ax.get_xlim()\n",
    "    xmin_frac = (xmin - x_min_lim - width) / (x_max_lim - x_min_lim)\n",
    "    xmax_frac = (xmax - x_min_lim + width) / (x_max_lim - x_min_lim)\n",
    "    ax.axhline(y=df[f'{moral_axis}_label'].mean(),\n",
    "               xmin=xmin_frac,\n",
    "               xmax=xmax_frac,\n",
    "               color='black',\n",
    "               linestyle='--')\n",
    "    \n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "ax.tick_params(axis='y', labelsize=13)\n",
    "ax.set_xticks(np.arange(len(keys.MORAL_AXES)))\n",
    "xtick_labels = keys.MORAL_AXES_LABELS_PLOT\n",
    "xtick_labels[4] = 'Relational\\nObligation'\n",
    "xtick_labels[5] = 'Social\\nNorms'\n",
    "ax.set_xticklabels(bold_text(keys.MORAL_AXES_LABELS_PLOT), fontsize=13)\n",
    "\n",
    "for idx, (model, model_label) in enumerate(zip(keys.MODELS, keys.MODEL_LABELS_PLOT)):\n",
    "    ax.bar(5, 0, color=f'C{idx}', label=bold_text(model_label))\n",
    "\n",
    "fig.legend(loc='center left',\n",
    "           bbox_to_anchor=(0.1, 0.73),\n",
    "           prop={'size': 10},\n",
    "           ncol=1)\n",
    "\n",
    "ax.set_ylabel(bold_text('Fraction of Samples'), fontsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('fig5_moral_themes.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5dfff8-5c7a-4d87-9e1a-86cca6a75b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, moral_model_col in moral_model_cols_dict.items():\n",
    "    print(model, df[moral_model_col].mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4df231e-159e-4255-9e9d-2855316c8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, moral_model_col in moral_model_cols_dict.items():\n",
    "    print(model, df[moral_model_col].sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66e7d8-239a-404c-839f-50526c531b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, moral_model_col in moral_model_cols_dict.items():\n",
    "    mean_correct = (df[moral_model_col].values == moral_labels_df.values).sum(axis=1).mean()\n",
    "    print(model, mean_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
